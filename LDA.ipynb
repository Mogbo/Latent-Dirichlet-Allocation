{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qfd_l8FHW1FC",
        "colab": {}
      },
      "source": [
        "%load_ext Cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ULoKlJ4vXmw_",
        "outputId": "2c40ec17-bf69-4951-842f-0ad20ed33e57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download(\"reuters\")\n",
        "nltk.download(\"stopwords\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q98u7-cOW1FV",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import reuters\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "ps = PorterStemmer()\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "stopwords_stem = list(map(ps.stem, stopwords))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2tLabjEYW1Fk"
      },
      "source": [
        "### Explore Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s9ywT1OPW1Fp",
        "outputId": "8fb8e3f7-287e-4d4e-ad42-de986a2bedc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(reuters.fileids())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JgxbpJ2dW1GG",
        "colab": {}
      },
      "source": [
        "corpus = [None]*len(reuters.fileids())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FVOEPLNwW1Gc",
        "colab": {}
      },
      "source": [
        "for i in range(len(corpus)):\n",
        "    corpus[i] = \" \".join(list(map(ps.stem, reuters.words(reuters.fileids()[i]))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j-mz_Ep2W1Gl",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "# pipe = Pipeline([('count', CountVectorizer(stop_words=stopwords_stem)),\n",
        "#                  ('tfid', TfidfTransformer(use_idf=False))]).fit(corpus)\n",
        "pipe = Pipeline([('count', CountVectorizer(stop_words=stopwords_stem))]).fit(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L43oyULHW1Gy",
        "colab": {}
      },
      "source": [
        "X = pipe.transform(corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B_ZikDKOW1G4",
        "colab": {}
      },
      "source": [
        "#pipe.named_steps['count'].vocabulary_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lNQTVT3QW1G-",
        "outputId": "3678845c-5d23-4253-c386-a24dabb1249d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10788, 23218)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "535cWPrFW1HL"
      },
      "source": [
        "#### LDA Gibbs Explore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R_YHx_TwW1HX"
      },
      "source": [
        "### Implement LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S9hOzavXW1HZ"
      },
      "source": [
        "#### Convert X to np.array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PtFYLbEdW1Ha",
        "colab": {}
      },
      "source": [
        "X = X.toarray().astype(np.intc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MTYUAoJHW1Hg",
        "colab": {}
      },
      "source": [
        "# for i in range(X.shape[0]):\n",
        "#     X[i,:][X[i,:]>0] = 1\n",
        "#     # for j in range(X.shape[1]):\n",
        "#     #     if (X[i,j] > 0):\n",
        "#     #         X[i,j] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LjnaS3U1W1Ho",
        "colab": {}
      },
      "source": [
        "num_topics=5\n",
        "alpha_=0.1\n",
        "gamma_=0.01\n",
        "random_state=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C8quKZ7FW1H1",
        "colab": {}
      },
      "source": [
        "word_topics = np.random.choice(np.arange(0, num_topics), size=X.shape).astype(np.intc)\n",
        "    \n",
        "# Nullify assignments for words that have zero count. We use a topic assignment, num_topics + 1, as a placeholder \n",
        "for i in range(word_topics.shape[0]):\n",
        "    word_topics[i][(X[i].flatten() == 0)] = num_topics + 1\n",
        "\n",
        "# Now we obtain local document topic counts\n",
        "doc_topics = np.empty((X.shape[0], num_topics),dtype=np.intc)\n",
        "for topic in range(num_topics):\n",
        "    doc_topics[:,topic] = np.sum(np.array((word_topics == topic), dtype=np.intc) * X, axis=1)\n",
        "\n",
        "# Now we obtain global document topic word counts\n",
        "topic_words_global = np.empty((num_topics, X.shape[1]), dtype=np.intc)\n",
        "for topic in range(num_topics):\n",
        "    topic_words_global[topic, :] = np.sum(np.array((word_topics == topic), dtype=np.intc) * X, axis=0)    \n",
        "\n",
        "# Calclate and Set Parameters\n",
        "sum_Vm = topic_words_global.sum(axis=1)\n",
        "V=X.shape[1]\n",
        "num_iter = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E-GOtz8BW1H-",
        "outputId": "465d0049-d393-40ed-fd14-a2066bd14821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "doc_topics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 66,  93,  75, 115,  98],\n",
              "       [ 17,  17,  14,   8,  14],\n",
              "       [ 19,  25,  27,  32,  18],\n",
              "       ...,\n",
              "       [  3,   4,   6,   3,   4],\n",
              "       [ 21,  16,   8,  14,   8],\n",
              "       [  8,   6,   4,   2,  11]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P1oVSSYxcVWc",
        "outputId": "31be06a6-08a3-45a5-a1d5-a84c6b4b0ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "topic_words_global"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  97, 2101,    2, ...,    1,    1,    0],\n",
              "       [  62, 2084,    2, ...,    0,    0,    0],\n",
              "       [  75, 1992,    1, ...,    0,    0,    0],\n",
              "       [  70, 2025,    2, ...,    1,    0,    0],\n",
              "       [  60, 2075,    4, ...,    1,    0,    1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NCUotN-McfwO",
        "outputId": "40b38d2b-c883-4f9c-fb3f-6a88d43d277d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 2, 0, ..., 0, 0, 0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sgl8nAw2cmSR",
        "colab": {}
      },
      "source": [
        "sum_Vm = sum_Vm.astype(np.intc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xul2QYwkW1IH",
        "colab": {}
      },
      "source": [
        "%%cython\n",
        "import numpy as np\n",
        "cimport numpy as np\n",
        "cimport cython\n",
        "\n",
        "@cython.boundscheck(False)  # turn off array bounds check\n",
        "@cython.wraparound(False)   # turn off negative indices ([-1,-1])\n",
        "def train(int[:,::1] X, int[:,::1] word_topics, int[:,::1] doc_topics, int[:,::1] topic_words_global, int[::1] sum_Vmb,\n",
        "          long long V, long long num_topics, double alpha_, double gamma_, long long random_state, long long num_iter):    \n",
        "    \n",
        "    cdef int N_i, word_count_iv, topic_ij, topic_iv, topic_ass, temp_count, num_docs\n",
        "    cdef double const_, theta_normalizer, beta_normalizer, log_like_sum\n",
        "    cdef int[::1] sum_Vm\n",
        "    cdef double[:,::1] theta\n",
        "    cdef double[:,::1] beta\n",
        "    cdef double[:,::1] loglike\n",
        "    \n",
        "    num_docs = X.shape[0]\n",
        "    \n",
        "    for it in range(num_iter):\n",
        "        for i in range(word_topics.shape[0]): # i is doc index: word_topics.shape[0] is num_docs\n",
        "            N_i = np.sum(X[i]) # Number of words in document i\n",
        "\n",
        "            for v in range(V): # v is word index: word_topics.shape[1] is V\n",
        "\n",
        "                word_count_iv = X[i,v]\n",
        "                if word_count_iv == 0:\n",
        "                    continue\n",
        "\n",
        "                # We want to re-assign word_topic_iv\n",
        "                # First we decrement document-level statistics\n",
        "                topic_iv = word_topics[i,v] # topic assigned to word iv\n",
        "                #print(i,v,topic_iv)\n",
        "\n",
        "                # Decrement document_topics\n",
        "                doc_topics[i, topic_iv] -= word_count_iv\n",
        "                doc_topic_i = doc_topics[i].copy()\n",
        "                #print(\"doc_topic_i\", np.array(doc_topic_i))\n",
        "                #doc_topic_i[topic_iv] -= word_count_iv\n",
        "\n",
        "                # Decrement global topic_words\n",
        "                topic_words_global[topic_iv, v] -= word_count_iv\n",
        "                topic_words_v = topic_words_global[:,v].copy()\n",
        "                #print(\"topic_words_v\", np.array(topic_words_v))\n",
        "                #topic_words_v[topic_iv] -= word_count_iv\n",
        "\n",
        "                # Because of sum_Vm we also need to update the global count\n",
        "                #topic_words_global[:,v] = topic_words_v\n",
        "                #topic_words_global[k,v]\n",
        "                #for idx in range(topic_words_global.shape[0]):\n",
        "                #    topic_words_global[idx,v] = topic_words_v[idx]\n",
        "\n",
        "                # How much doc likes topic, dim K\n",
        "                partA = np.zeros(num_topics)\n",
        "                for k in range(num_topics):\n",
        "                    partA[k] = (doc_topic_i[k] + alpha_)/(N_i - 1 + num_topics*alpha_) # dim K\n",
        "                #partA = (doc_topic_i + alpha_)/(N_i - 1 + num_topics*alpha_) # dim K\n",
        "                #print(partA)\n",
        "\n",
        "                # How much each topic like word, dim K\n",
        "                #partB = (topic_words_v + gamma_)/(sum_Vm + V*gamma_)\n",
        "\n",
        "                # How much each topic like word\n",
        "                #sum_Vm = np.sum(topic_words_global, axis=1)\n",
        "\n",
        "                sum_Vm = np.zeros(num_topics, dtype=np.intc) \n",
        "                # sum_Vm[k] is number of times topic k has been assigned in the corpus. \n",
        "                for k in range(num_topics):\n",
        "                    sum_Vm[k] = np.sum(topic_words_global[k,:])\n",
        "\n",
        "                partB = np.zeros(num_topics)\n",
        "                for k in range(num_topics):\n",
        "                    partB[k] = (topic_words_v[k] + gamma_)/(sum_Vm[k] + V*gamma_)\n",
        "\n",
        "\n",
        "                #prob_unnormalized\n",
        "                prob_unorm = partA*partB\n",
        "                const_ = np.sum(prob_unorm)\n",
        "                prob = prob_unorm/const_\n",
        "\n",
        "                # Draw a topic based on prob\n",
        "                topic_ass = np.random.choice(np.arange(0, num_topics), p=prob)\n",
        "\n",
        "                #Update Counts\n",
        "                #doc_topic_i[topic_ass] += word_count_iv\n",
        "                #topic_words_i[topic_ass] += word_count_iv\n",
        "\n",
        "                doc_topics[i, topic_ass] += word_count_iv\n",
        "                topic_words_global[topic_ass, v] += word_count_iv\n",
        "                \n",
        "                doc_topic_i = doc_topics[i].copy()\n",
        "                #print(\"doc_topic_i\", np.array(doc_topic_i))\n",
        "                \n",
        "                topic_words_v = topic_words_global[:,v].copy()\n",
        "                #print(\"topic_words_v\", np.array(topic_words_v))\n",
        "\n",
        "                #doc_topics[i,:] = doc_topic_i\n",
        "                #for idx in range(doc_topics.shape[1]):\n",
        "                #    doc_topics[i,idx] = doc_topic_i[idx]\n",
        "\n",
        "\n",
        "                #topic_words_global[:,v] = topic_words_v \n",
        "                #for idx in range(topic_words_global.shape[0]):\n",
        "                #    topic_words_global[idx,v] = topic_words_v[idx]\n",
        "\n",
        "                #Update assignment\n",
        "                word_topics[i,v] = topic_ass\n",
        "                #import pdb; pdb.set_trace()\n",
        "#     return topic_words_global, doc_topics\n",
        "            \n",
        "        theta = np.zeros((word_topics.shape[0], num_topics), dtype=np.float_)\n",
        "        beta = np.zeros((num_topics, V), dtype=np.float_)\n",
        "\n",
        "        # Calculate theta (Sec 7)\n",
        "        for i in range(theta.shape[0]):\n",
        "            for k in range(theta.shape[1]):\n",
        "\n",
        "                #np.sum(np.array((word_topics[i,:] == k), dtype=np.intc) * X, axis=1)\n",
        "                temp_count = 0\n",
        "                for v in range(V):\n",
        "                    if (word_topics[i,v] == k):\n",
        "                        temp_count += X[i,v]\n",
        "                theta[i,k] = (temp_count + alpha_) # Note that this not normalized. we will do outside the loop\n",
        "\n",
        "            # Now we normalize\n",
        "            theta_normalizer = num_topics*alpha_ + np.sum(theta[i,:])       \n",
        "            for k in range(theta.shape[1]):\n",
        "                theta[i,k] = theta[i,k]/theta_normalizer\n",
        "\n",
        "        # Calculate beta (Sec 8)\n",
        "        for k in range(beta.shape[0]):\n",
        "            for v in range(V):\n",
        "\n",
        "                # np.sum(np.array((word_topics[:,v] == k), dtype=np.intc) * X, axis=0)\n",
        "                temp_count = 0\n",
        "                for i in range(num_docs):\n",
        "                    if (word_topics[i,v] == k):\n",
        "                        temp_count += X[i,v]\n",
        "                beta[k,v] = (temp_count + gamma_)\n",
        "\n",
        "            # Now we normalize\n",
        "            beta_normalizer = V*gamma_ + np.sum(beta[:,v])\n",
        "            for v in range(V):\n",
        "                beta[k,v] = beta[k,v]/beta_normalizer\n",
        "\n",
        "        log_like_sum = 0\n",
        "        if ((it + 1) % 4 == 1 or it == 0 or it==(num_iter-1)):\n",
        "            #if True:\n",
        "            for i in range(X.shape[0]):\n",
        "                for j in range(X.shape[1]):\n",
        "                    topic_ij = word_topics[i,j]\n",
        "                    #log_like[i,v] = np.log(beta[topic_iv])\n",
        "                    log_like = X[i,j]*(np.log(beta[topic_ij,j]) + np.log(theta[i, topic_ij]))\n",
        "                    log_like_sum += log_like\n",
        "\n",
        "            print(log_like_sum)\n",
        "    \n",
        "    return beta, theta, topic_words_global, doc_topics\n",
        "\n",
        "            #import pdb; pdb.set_trace()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jfe2s1L6W1IM"
      },
      "source": [
        "#### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8zTaDeOkW1IO",
        "outputId": "dabcd99a-58f5-47f5-da36-e534baa733b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "import time\n",
        "time0 = time.time()\n",
        "beta, theta, topic_words_global, doc_topics = train(X, word_topics, doc_topics, topic_words_global, sum_Vm, V, num_topics, alpha_, gamma_, random_state, num_iter)\n",
        "print(time.time() -time0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-1851575.2663594345\n",
            "-1347160.6884145404\n",
            "-907452.9994037489\n",
            "-665135.9190033425\n",
            "-542154.7068081292\n",
            "-474198.0026794867\n",
            "-434819.6054196948\n",
            "8274.408559322357\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wjK-AqzoW1IR",
        "outputId": "2682acfb-7853-4ee2-e498-d8ee1c89320e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "np.array(doc_topics)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,  22,  78, 347],\n",
              "       [  0,  39,   0,  18,  13],\n",
              "       [  9,   1,  14,  48,  49],\n",
              "       ...,\n",
              "       [ 20,   0,   0,   0,   0],\n",
              "       [ 65,   0,   0,   2,   0],\n",
              "       [ 31,   0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7kGtKpOwW1IX",
        "colab": {}
      },
      "source": [
        "#print(topic_words_global[:,77])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_o5st6DWW1If",
        "outputId": "7732d0db-7787-4e67-e6e2-f691f4a0c763",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "word_topics[0,155]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A9sTpZ6gW1Il",
        "outputId": "d2e50574-1c4f-477a-eda1-1bd8a2ca4e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "np.array(topic_words_global)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  43, 7544,    9, ...,    3,    1,    0],\n",
              "       [ 112, 2314,    0, ...,    0,    0,    0],\n",
              "       [  80,  304,    1, ...,    0,    0,    0],\n",
              "       [ 115,  115,    1, ...,    0,    0,    0],\n",
              "       [  14,    0,    0, ...,    0,    0,    1]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rtmD75fuam6d",
        "colab": {}
      },
      "source": [
        "# Calculate theta (Sec 7)\n",
        "for i in range(theta.shape[0]):\n",
        "    for k in range(theta.shape[1]):\n",
        "\n",
        "        #np.sum(np.array((word_topics[i,:] == k), dtype=np.intc) * X, axis=1)\n",
        "        temp_count = 0\n",
        "        for v in range(V):\n",
        "            if (word_topics[i,v] == k):\n",
        "                temp_count += X[i,v]\n",
        "        theta[i,k] = (temp_count + alpha_) # Note that this not normalized. we will do outside the loop\n",
        "\n",
        "    # Now we normalize\n",
        "    theta_normalizer = num_topics*alpha_ + np.sum(theta[i,:])       \n",
        "    for k in range(theta.shape[1]):\n",
        "        theta[i,k] = theta[i,k]/theta_normalizer\n",
        "\n",
        "# Calculate beta (Sec 8)\n",
        "for k in range(beta.shape[0]):\n",
        "    for v in range(V):\n",
        "\n",
        "        # np.sum(np.array((word_topics[:,v] == k), dtype=np.intc) * X, axis=0)\n",
        "        temp_count = 0\n",
        "        for i in range(num_docs):\n",
        "            if (word_topics[i,v] == k):\n",
        "                temp_count += X[i,v]\n",
        "        beta[k,v] = (temp_count + gamma_)\n",
        "\n",
        "    # Now we normalize\n",
        "    beta_normalizer = V*gamma_ + np.sum(beta[:,v])\n",
        "    for v in range(V):\n",
        "        beta[k,v] = beta[k,v]/beta_normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Usqa2lS-W1Io"
      },
      "source": [
        "###  Translate back to vector form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LgumelXZW1Ip",
        "colab": {}
      },
      "source": [
        "#pipe.named_steps['count'].vocabulary_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4dJFsUwFW1Is",
        "colab": {}
      },
      "source": [
        "inv_vocab = dict((v,k) for (k,v) in pipe.named_steps['count'].vocabulary_.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cOyhhvnfW1I1",
        "colab": {}
      },
      "source": [
        "def print_top_words(beta, num_words):\n",
        "    for i in range(beta.shape[0]):\n",
        "        str_ = \"Topic \" + str(i) + \": \"\n",
        "        for word_idx in beta[i].argsort()[::-1][:num_words]:\n",
        "            str_ = str_ + \" \" + inv_vocab[word_idx]\n",
        "        print(str_)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "90q9Nr_TW1I8",
        "outputId": "fec6354a-96e4-4456-aa4d-1538f269c63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "print_top_words(np.array(beta), 20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic 0:  vs mln ct 000 net dlr shr loss lt profit qtr rev year oper note inc sale 1986 31 share\n",
            "Topic 1:  said tonn 000 mln year export price wheat product dlr 1986 grain sugar last 87 per pct corn crop agricultur\n",
            "Topic 2:  said lt compani share dlr inc corp mln pct stock offer group co unit acquir sale oil acquisit ltd sharehold\n",
            "Topic 3:  pct said billion year bank mln dlr rate 1986 februari rise januari rose quarter last 1987 expect increas reserv month\n",
            "Topic 4:  said would trade market japan dollar offici price oil meet new govern countri could state say industri also told bank\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "khZv0Wf0lKRd",
        "outputId": "bbd567ce-5559-45c7-ae6d-ea66e2f909b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "for i in range(beta.shape[0]):\n",
        "  print(np.sort(beta[i])[::-1][:20]/np.sum(beta[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.07943213 0.06087173 0.04567738 0.04183443 0.03682695 0.02972331\n",
            " 0.02916877 0.0290745  0.01852718 0.01547168 0.01486723 0.01336998\n",
            " 0.01109082 0.01102982 0.00886712 0.00846231 0.00703715 0.0069096\n",
            " 0.00645488 0.00644379]\n",
            "[0.02542419 0.02296693 0.01876607 0.01810107 0.01111856 0.01028326\n",
            " 0.00891271 0.00839368 0.00793143 0.00766381 0.00687716 0.00644734\n",
            " 0.0064149  0.0060824  0.00595265 0.00557149 0.00543362 0.0053363\n",
            " 0.00523088 0.00497137]\n",
            "[0.04237644 0.02582501 0.02279461 0.02183375 0.02175983 0.01238775\n",
            " 0.01083559 0.01052516 0.00994865 0.00977126 0.00881532 0.00758838\n",
            " 0.00677534 0.00607564 0.00583912 0.00546956 0.00534145 0.00523797\n",
            " 0.00488812 0.00486348]\n",
            "[0.03778151 0.02428276 0.02315383 0.02068632 0.01836395 0.01765971\n",
            " 0.01615985 0.01175166 0.0114076  0.00800469 0.00795631 0.00795631\n",
            " 0.006822   0.00651558 0.0062898  0.00620916 0.0059995  0.00591349\n",
            " 0.00567157 0.00562319]\n",
            "[0.03358061 0.01306671 0.01169495 0.00894039 0.00605712 0.00575923\n",
            " 0.00574084 0.00552018 0.00539514 0.00476994 0.0046449  0.00463387\n",
            " 0.00461916 0.00458974 0.0043985  0.00427714 0.00426243 0.00423301\n",
            " 0.00422197 0.00418887]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VIDcvN6bQ7an"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XlxoPkBdW1JE",
        "outputId": "e4fb5c51-2f55-46f0-e0fa-f955b5d678b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot([1,5,9,13,17,21,25],[-1851575.2663594345, -1347160.6884145404, -907452.9994037489, -665135.9190033425, -542154.7068081292, \n",
        "                -474198.0026794867, -434819.6054196948], \"x\", MarkerSize=10)\n",
        "#plt.yscale(\"symlog\")\n",
        "#plt.ylim((-1*10**12, -1*10**8))\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Log-likelihood\")\n",
        "plt.savefig(\"LDA_gibbs.png\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}